import json
import chromadb
from sentence_transformers import SentenceTransformer
from pathlib import Path

def normalize_data(item, item_type):
    """JSON ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    tags = item.get("íƒœê·¸", "")
    if tags is None:
        tags = ""
        
    return {
        "id": str(item.get("id", item.get("ì´ë¦„", ""))) + "_" + item_type,
        "type": item_type,
        "title": item.get("ì´ë¦„", ""),
        "address": item.get("ì£¼ì†Œ", "ì œì£¼ë„"),
        "desc": item.get("ì†Œê°œ", ""),
        "tags": tags.replace("#", "").replace(" ", "").split(",") if tags else [],
        "content": f"{item.get('ì´ë¦„', '')}ì€(ëŠ”) {item.get('ì£¼ì†Œ', 'ì œì£¼ë„')}ì— ìœ„ì¹˜í•œ {item_type} ì¥ì†Œì…ë‹ˆë‹¤. {item.get('ì†Œê°œ', '')} ê´€ë ¨ íƒœê·¸: {tags}"
    }

def load_json_file(file_path):
    """JSON íŒŒì¼ì„ ë¡œë“œ"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

def create_vector_db():
    """ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„± (384ì°¨ì› ì„ë² ë”© ì‚¬ìš©)"""
    
    print("ğŸš€ ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“ JSON ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
    try:
        food_data = [normalize_data(x, "ìŒì‹") for x in load_json_file("visitjeju_food.json")]
        tour_data = [normalize_data(x, "ê´€ê´‘") for x in load_json_file("visitjeju_tour.json")]
        hotel_data = [normalize_data(x, "ìˆ™ì†Œ") for x in load_json_file("visitjeju_hotel.json")]
        event_data = [normalize_data(x, "í–‰ì‚¬") for x in load_json_file("visitjeju_event.json")]
        
        all_documents = food_data + tour_data + hotel_data + event_data
        print(f"âœ… ì´ {len(all_documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        return False

    # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (384ì°¨ì›)
    print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    try:
        embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # 3. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    print("ğŸ”¢ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        texts = [doc["content"] for doc in all_documents]
        embeddings = embedding_model.encode(texts, show_progress_bar=True)
        print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return False

    # 4. ChromaDB ìƒì„±
    print("ğŸ’¾ ChromaDBë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        # ìƒˆë¡œìš´ DB ê²½ë¡œ
        new_db_path = "./new_vector_store"
        client = chromadb.PersistentClient(path=new_db_path)
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œ
        collection_name = "jeju_place_info_384"
        try:
            existing_collection = client.get_collection(collection_name)
            client.delete_collection(collection_name)
            print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œë¨")
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "ì œì£¼ë„ ì—¬í–‰ ì •ë³´ (384ì°¨ì› ì„ë² ë”©)"}
        )
        
        print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ChromaDB ìƒì„± ì‹¤íŒ¨: {e}")
        return False

    # 5. ë°ì´í„° ì €ì¥
    print("ğŸ“ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    try:
        # ì¤‘ë³µ ì œê±°
        unique_ids = []
        unique_texts = []
        unique_embeddings = []
        unique_metadatas = []
        seen_ids = set()
        
        for i, doc in enumerate(all_documents):
            doc_id = doc["id"]
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_ids.append(doc_id)
                unique_texts.append(texts[i])
                unique_embeddings.append(embeddings[i].tolist())
                unique_metadatas.append({
                    "type": doc["type"],
                    "title": doc["title"],
                    "address": doc["address"]
                })
        
        print(f"ì¤‘ë³µ ì œê±° í›„ {len(unique_ids)}ê°œ ë¬¸ì„œ")
        
        # ë°°ì¹˜ë¡œ ì €ì¥
        batch_size = 1000
        for i in range(0, len(unique_ids), batch_size):
            end_idx = min(i + batch_size, len(unique_ids))
            
            collection.add(
                documents=unique_texts[i:end_idx],
                embeddings=unique_embeddings[i:end_idx],
                metadatas=unique_metadatas[i:end_idx],
                ids=unique_ids[i:end_idx]
            )
            
            print(f"ì €ì¥ ì§„í–‰: {end_idx}/{len(unique_ids)}")
        
        print(f"âœ… ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

    # 6. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    try:
        test_results = collection.query(
            query_texts=["ì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ"],
            n_results=3
        )
        
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        for i, doc in enumerate(test_results["documents"][0]):
            meta = test_results["metadatas"][0][i]
            print(f"  {i+1}. {meta.get('title')} ({meta.get('type')})")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
        return False

    print(f"""
ğŸ‰ ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„± ì™„ë£Œ!

ğŸ“ DB ê²½ë¡œ: {new_db_path}
ğŸ“ ì»¬ë ‰ì…˜ëª…: {collection_name}
ğŸ“ ë¬¸ì„œ ìˆ˜: {len(unique_ids)}
ğŸ“ ì„ë² ë”© ì°¨ì›: 384

ì´ì œ smart_chatbot.pyì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•˜ì„¸ìš”:
1. db_path = "./new_vector_store"
2. ì„ë² ë”© ëª¨ë¸: "snunlp/KR-SBERT-V40K-klueNLI-augSTS"
""")
    
    return True

if __name__ == "__main__":
    create_vector_db()